{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions were loaded.\n"
     ]
    }
   ],
   "source": [
    "def delete_folder(directory_dir, size= batch_rand * factor_no):\n",
    "    folder_list = os.listdir(directory_dir)\n",
    "    folder_dir = [os.path.join(directory_dir,i) for i in folder_list if len(os.listdir(os.path.join(directory_dir,i))) <= size ]\n",
    "    for folder in folder_dir:\n",
    "        print('deleting ' + folder + ' now...')\n",
    "        shutil.rmtree(folder)\n",
    "    print('completed!')\n",
    "\n",
    "\n",
    "def cal_weight(class_name_list,IN_DIR):\n",
    "    amounts_of_class_dict = {}\n",
    "    mx = 0\n",
    "    tot = 0\n",
    "    for class_name in class_name_list:\n",
    "        class_dir = IN_DIR + os.sep + class_name\n",
    "        file_list = os.listdir(class_dir)\n",
    "        amounts_of_class_dict[class_name] = len(file_list)\n",
    "        if mx < len(file_list):\n",
    "            mx = len(file_list)\n",
    "    class_weights = {}\n",
    "    count = 0\n",
    "    for class_name in class_name_list:\n",
    "        class_weights[count] = round(float(math.pow(amounts_of_class_dict[class_name]/mx, -1)),2)\n",
    "        # weight = 1/(data nubmer/ MAX data number)\n",
    "        count += 1\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def batch_iter(data, labels, batch_size, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X = shuffled_data[start_index: end_index]\n",
    "                y = shuffled_labels[start_index: end_index]\n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "\n",
    "# Grad-CAM visualisation\n",
    "# Grad-CAM for batched images\n",
    "def grad_cam_batch(Input_model, Images, Classes, Layer_name, NULL_list, X_size, Y_size):\n",
    "    if Y_size == 1:\n",
    "        Loss = tf.gather_nd(Input_model.output, np.dstack([range(Images.shape[0]), Classes])[0])\n",
    "        Layer_output = Input_model.get_layer(Layer_name).output\n",
    "        Grads = K.gradients(Loss, Layer_output)[0]\n",
    "        gradient_fnc = K.function([Input_model.input, tf.constant([0])], [Layer_output, Grads])\n",
    "\n",
    "        conv_output, Grads_val = gradient_fnc([Images, tf.constant([0])])    \n",
    "        weights = np.mean(Grads_val, axis=2)\n",
    "        cams = np.einsum('ijk,ik->ij', conv_output.transpose(0,2,1), weights)\n",
    "\n",
    "        # Process CAMs\n",
    "        new_cams = np.zeros(X_size)\n",
    "        for i in range(Images.shape[0]):\n",
    "            cams[i] = np.maximum(cams[i], 0.)\n",
    "            if np.max(cams[i]) != 0.:\n",
    "                if math.isfinite(cams[i][0]):\n",
    "                    new_cams += cams[i] / np.max(cams[i])\n",
    "        return new_cams\n",
    "\n",
    "    else:\n",
    "        Loss = tf.gather_nd(Input_model.output, np.dstack([range(Images.shape[0]), Classes])[0])\n",
    "        Layer_output = Input_model.get_layer(Layer_name).output\n",
    "        Grads = K.gradients(Loss, Layer_output)[0]\n",
    "        gradient_fnc = K.function([Input_model.input, tf.constant([0])], [Layer_output, Grads])\n",
    "\n",
    "        conv_output, Grads_val = gradient_fnc([Images, tf.constant([0])])    \n",
    "        weights = np.mean(Grads_val, axis=(2, 3))\n",
    "        cams = np.einsum('ijkl,il->ijk', conv_output.transpose(0,2,3,1), weights)\n",
    "        for i in range(len(NULL_list)):\n",
    "            cams[:,NULL_list[i][0],NULL_list[i][1]] = 0\n",
    "\n",
    "        # Process CAMs\n",
    "        new_cams = np.zeros((X_size, Y_size))\n",
    "        for i in range(Images.shape[0]):\n",
    "            cams[i] = np.maximum(cams[i], 0.)\n",
    "            if np.max(cams[i]) != 0.:\n",
    "                if math.isfinite(cams[i][0][0]):\n",
    "                    new_cams += cams[i] / np.max(cams[i])\n",
    "        return new_cams\n",
    "\n",
    "def grad_batch(Input_model, Images, Classes, Layer_name, NULL_list, X_size, Y_size):\n",
    "    if Y_size == 1:\n",
    "        Loss = tf.gather_nd(Input_model.output, np.dstack([range(Images.shape[0]), Classes])[0])\n",
    "        Grads = K.gradients(Loss, Input_model.input)[0]\n",
    "        gradient_fnc = K.function([Input_model.input], [Grads])\n",
    "        Grads_val = gradient_fnc([Images])[0]\n",
    "        new_cams = np.zeros(X_size)\n",
    "        for i in range(Images.shape[0]):\n",
    "            new_cams +=  Grads_val[i][0]/(np.abs(Grads_val[i][0]).max())\n",
    "        return new_cams\n",
    "\n",
    "    else:\n",
    "        Loss = tf.gather_nd(Input_model.output, np.dstack([range(Images.shape[0]), Classes])[0])\n",
    "        Grads = K.gradients(Loss, Input_model.input)[0]\n",
    "        gradient_fnc = K.function([Input_model.input], [Grads])\n",
    "        Grads_val = gradient_fnc([Images])[0]\n",
    "        new_cams = np.zeros((X_size, Y_size))\n",
    "        for i in range(Images.shape[0]):\n",
    "            new_cams +=  Grads_val[i][0]/(np.abs(Grads_val[i][0]).max())\n",
    "        return new_cams\n",
    "\n",
    "def Grad_imaging(cam):\n",
    "    jetcam = cv2.applyColorMap(np.uint8(256 * cam), cv2.COLORMAP_JET)\n",
    "    jetcam = cv2.cvtColor(jetcam, cv2.COLOR_BGR2RGB)\n",
    "    return jetcam\n",
    "\n",
    "def Matthew(TP, TN, FP, FN):\n",
    "    return ((TP*TN)-(FP*FN))/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "\n",
    "\n",
    "def Grad_Cam_imaging(cam):\n",
    "    jetcam = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    jetcam = cv2.cvtColor(jetcam, cv2.COLOR_BGR2RGB)\n",
    "    return jetcam\n",
    "\n",
    "print(\"Functions were loaded.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TCGA data download @ 2019/10/22\n",
    "\n",
    "###############\n",
    "\n",
    "file type == HT-Seq, FPKM\n",
    "sample type == primary tumor \n",
    "    project TCGA\n",
    "        FILES 9760\n",
    "        CASES 9679\n",
    "        FILE SIZE 5.06 GB\n",
    "    project non-TCGA; TARGET, CPTAC, CGCI, HCMI\n",
    "        FILES 882\n",
    "        CASES 879\n",
    "        FILE SIZE 285.49 MB\n",
    "sample type == tumor \n",
    "        FILES 522\n",
    "        CASES 522\n",
    "        FILE SIZE 157.3 MB\n",
    "sample type == Primary Blood Derived Cancer - Bone Marrow\n",
    "        FILES 1528\n",
    "        CASES 1499\n",
    "        FILE SIZE 499.51 MB\n",
    "sample type == Primary Blood Derived Cancer - Peripheral blood\n",
    "        FILES 369\n",
    "        CASES 365\n",
    "        FILE SIZE 158.12 MB\n",
    "        \n",
    "Primary tumors: total 13061 samples\n",
    "    TCGA      9,911 samples\n",
    "    patho    11,997 samples\n",
    "\n",
    "###############\n",
    "\n",
    "sample type == solid tissue normal        \n",
    "        FILES 943\n",
    "        CASES 943\n",
    "        FILE SIZE 448.91 MB\n",
    "\n",
    "###############\n",
    "\n",
    "sample type == Metastatic\n",
    "        FILES 395\n",
    "        CASES 395\n",
    "        FILE SIZE 201.32 MB\n",
    "        \n",
    "###############\n",
    "\n",
    "# manifest files\n",
    "\n",
    "gdc_manifest_TCGA_primary_tumor.txt\n",
    "gdc_manifest_non_TCGA_primary_tumor.txt\n",
    "gdc_manifest_tumor.txt\n",
    "gdc_manifest_Primary_Blood_Derived_Cancer_marrow.txt\n",
    "gdc_manifest_Primary_Blood_Derived_Cancer_peripheral.txt\n",
    "gdc_manifest_TCGA_normal.txt\n",
    "gdc_manifest_meta.txt\n",
    "\n",
    "# information files\n",
    "\n",
    "files.json\n",
    "clinical.tsv\n",
    "\n",
    "# Download files from GDC site with gdc-client application supplied by GDC\n",
    "# https://gdc.cancer.gov/access-data/gdc-data-transfer-tool\n",
    "# gdc-client was put at home_dir\n",
    "###############\n",
    "# run these commands at home_dir/expression_data\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_Primary_Blood_Derived_Cancer_marrow.txt\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_tumor.txt\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_non_TCGA_primary_tumor.txt\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_TCGA_primary_tumor.txt\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_Primary_Blood_Derived_Cancer_peripheral.txt\n",
    "\n",
    "###############\n",
    "# run the command at home_dir/normal\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_Primary_Blood_Derived_Cancer_peripheral.txt\n",
    "\n",
    "###############\n",
    "# run the command at home_dir/normal\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_TCGA_normal.txt\n",
    "\n",
    "gdc_manifest_mirna_normal.txt\n",
    "\n",
    "###############\n",
    "# run the command at home_dir/mirna_normal\n",
    "../gdc-client download -m ../GDC_files/gdc_manifest_mirna_normal.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing very rare classes (less than or equal to 'factor_no' samples per class)\n",
    "\n",
    "if imaging_pathway == 2 or imaging_pathway == 4:\n",
    "    delete_folder(directory_dir = data_dir, size= batch_rand * factor_no)\n",
    "\n",
    "n_categories = len(os.listdir(data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading KEGG pathway annonation\n",
    "\n",
    "if not os.path.exists(home_dir + '/KEGG'):\n",
    "    os.mkdir(home_dir + '/KEGG')\n",
    "if not os.path.exists(home_dir + '/info'):\n",
    "    os.mkdir(home_dir + '/info')\n",
    "\n",
    "url_anno = 'http://rest.kegg.jp/link/hsa/pathway'\n",
    "url_gene = 'http://rest.kegg.jp/list/hsa'\n",
    "url_path = 'http://rest.kegg.jp/list/pathway/hsa'\n",
    "url_ncbi = 'http://rest.kegg.jp/conv/hsa/ncbi-geneid'\n",
    "\n",
    "try:\n",
    "    r_a = requests.get(url_anno)\n",
    "    r_g = requests.get(url_gene)\n",
    "    r_p = requests.get(url_path)\n",
    "    r_n = requests.get(url_ncbi)\n",
    "\n",
    "    with open(home_dir + '/KEGG/link.tsv', mode='w') as f_a:\n",
    "        f_a.write(r_a.text)\n",
    "    with open(home_dir + '/KEGG/genes.tsv', mode='w') as f_g:\n",
    "        f_g.write(r_g.text)\n",
    "    with open(home_dir + '/KEGG/pathway.tsv', mode='w') as f_p:\n",
    "        f_p.write(r_p.text)\n",
    "    with open(home_dir + '/KEGG/ncbi_geneid.tsv', mode='w') as f_n:\n",
    "        f_n.write(r_n.text)\n",
    "\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Making gene ID conversion list with R language\n",
    "# RUN on R-studio etc.\n",
    "\n",
    "```{r}\n",
    "library(tidyverse)\n",
    "library(biomaRt)\n",
    "ensembl=useMart(\"ensembl\")\n",
    "ensembl = useDataset(\"hsapiens_gene_ensembl\", mart=ensembl)\n",
    "# Changing ENSG id to NCBI gene id \n",
    "df_FPKM = read_tsv('/mnt/IKEGAMI/R/expression_profile/info/FPKM_test.tsv', col_names = F)\n",
    "\n",
    "# remove version number\n",
    "ensg = df_FPKM[,1][[1]]\n",
    "ensg.no_version = sapply(strsplit(as.character(ensg),\"\\\\.\"),\"[[\",1)\n",
    "\n",
    "# convert EnsG -> Entrez Gene ID\n",
    "BIOMART = getBM(attributes = c('ensembl_gene_id', 'external_gene_name', 'gene_biotype', 'entrezgene_id'), checkFilters=FALSE, values=ensg.no_version, mart=ensembl)\n",
    "\n",
    "write_tsv(BIOMART, \"/mnt/IKEGAMI/R/expression_profile/info/ENSG_to_NCBI.tsv\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selected, top-10, 309 genes\n",
    "\n",
    "###### Making template from KEGG pathway information files\n",
    "# only primary tumor samples\n",
    "# TCGA project: 9,911 samples\n",
    "# major pathogolic types from GDC database: 11,997 samples\n",
    "\n",
    "# 60483 features\n",
    "# using only 308 genes selected by Pagging method\n",
    "\n",
    "# making 308 x 1 pxls images with 32 patterns\n",
    "\n",
    "if not os.path.exists(home_dir + '/array_select_309'):\n",
    "    os.mkdir(home_dir + '/array_select_309/')\n",
    "if not os.path.exists(home_dir + '/image_select_309'):\n",
    "    os.mkdir(home_dir + '/image_select_309')\n",
    "if not os.path.exists(home_dir + '/select_patho_309'):\n",
    "    os.mkdir(home_dir + '/select_patho_309')\n",
    "\n",
    "X_size_ = X_size\n",
    "Y_size_ = Y_size\n",
    "X_size = 309\n",
    "Y_size = 1\n",
    "\n",
    "df_genes = pd.read_table(home_dir + '/KEGG/genes.tsv', header=None)\n",
    "df_link = pd.read_table(home_dir + '/KEGG/link.tsv', header=None)\n",
    "df_pathway = pd.read_table(home_dir + '/KEGG/pathway.tsv', header=None)\n",
    "df_ncbi_geneid = pd.read_table(home_dir + '/KEGG/ncbi_geneid.tsv', header=None)\n",
    "df_ENSG = pd.read_table(home_dir + '/info/ENSG_to_NCBI.tsv', header=0, index_col=0)\n",
    "df_FPKM = pd.read_table(home_dir + '/info/FPKM_test.tsv', header=None)\n",
    "df_select = pd.read_table(home_dir + '/info/309_genes.tsv', header=None)\n",
    "\n",
    "\n",
    "df_FPKM.columns = ['ENSG', 'FPKM']\n",
    "df_FPKM[\"NCBI\"] = \"NA\"\n",
    "df_FPKM[\"Type\"] = \"NA\"\n",
    "df_FPKM[\"ENSG_no_ver\"] = \"NA\"\n",
    "df_FPKM[\"FPKM_log2\"] = np.log2(df_FPKM[\"FPKM\"]+1)\n",
    "df_FPKM[\"Gene_name\"] = \"NA\"\n",
    "df_FPKM = df_FPKM[df_FPKM['ENSG'].str.split('.', expand=True)[0].isin(list(df_select[0]))]\n",
    "\n",
    "for i in range(len(df_FPKM[\"ENSG\"])):\n",
    "    if i % 500 ==0:\n",
    "        print(i)\n",
    "    df_FPKM.iloc[i,4] = df_FPKM.iloc[i,0].split(\".\")[0]\n",
    "    try:\n",
    "        tmp = df_ENSG.loc[df_FPKM.iloc[i,4]]\n",
    "    except:\n",
    "        continue\n",
    "    if tmp[\"entrezgene_id\"].size == 1:\n",
    "        df_FPKM.iloc[i,2] = tmp.iloc[2]\n",
    "        df_FPKM.iloc[i,3] = tmp.iloc[1]\n",
    "        df_FPKM.iloc[i,6] = tmp.iloc[0]\n",
    "    else:\n",
    "        for j in range(tmp[\"gene_biotype\"].size):\n",
    "            tmp2 = pd.Series([df_FPKM.iloc[i,0], df_FPKM.iloc[i,1], tmp.iloc[j,2], tmp.iloc[j,1], df_FPKM.iloc[i,4], df_FPKM.iloc[i,5], tmp.iloc[j,0]], index=['ENSG','FPKM','NCBI','Type','ENSG_no_ver','FPKM_log2','Gene_name'])\n",
    "            df_FPKM = df_FPKM.append([tmp2])\n",
    "\n",
    "df_ncbi_geneid.columns = ['KEGG', 'NCBI']\n",
    "for i in range(len(df_ncbi_geneid)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    df_ncbi_geneid.iloc[i,0] = df_ncbi_geneid.iloc[i,0].lstrip(\"hsa:\")\n",
    "    df_ncbi_geneid.iloc[i,1] = df_ncbi_geneid.iloc[i,1].lstrip(\"ncbi-geneid:\")\n",
    "\n",
    "df_ncbi_geneid_ = df_ncbi_geneid.set_index('KEGG')\n",
    "df_link.columns = ['KEGG_PATH', 'KEGG_GENE']\n",
    "df_link[\"NCBI\"] = \"NA\"\n",
    "\n",
    "for i in range(len(df_link)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    df_link.iloc[i,0] = df_link.iloc[i,0].lstrip(\"path:hsa\")\n",
    "    df_link.iloc[i,1] = df_link.iloc[i,1].lstrip(\"hsa:\")\n",
    "    try:\n",
    "        df_link.iloc[i,2] = df_ncbi_geneid_.loc[str(int(df_link.iloc[i,1]))].iloc[0]\n",
    "    except:\n",
    "        df_link.iloc[i,2] = \"NA\"\n",
    "\n",
    "df_FPKM_ = df_FPKM.copy()\n",
    "df_link_ = df_link.copy()\n",
    "df_link_[\"FPKM_log2\"] = 0\n",
    "df_link_[\"ENSG_no_var\"] = \"NA\"\n",
    "df_link_[\"Gene_name\"] = \"NA\"\n",
    "df_link_[\"Type\"] = \"NA\"\n",
    "df_link_[\"index\"] = 0\n",
    "df_FPKM_[\"Check\"] = 0\n",
    "df_FPKM_ = df_FPKM_.set_index('NCBI')\n",
    "\n",
    "for i in range(len(df_link_)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    try:\n",
    "        j = float(df_link_.iloc[i,2])\n",
    "        tmp = df_FPKM_.loc[j]\n",
    "        df_link_.iloc[i,3] = tmp.iloc[4]\n",
    "        df_link_.iloc[i,4] = tmp.iloc[3]\n",
    "        df_link_.iloc[i,5] = tmp.iloc[5]\n",
    "        df_link_.iloc[i,6] = tmp.iloc[2]\n",
    "        df_link_.iloc[i,7] = tmp.iloc[1]\n",
    "        df_FPKM_.loc[j,df_FPKM_.columns[6]] = 1\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_FPKM__ = df_FPKM_[df_FPKM_[\"Check\"] == 0].drop_duplicates(subset='ENSG', keep='last')\n",
    "df_FPKM___ = df_FPKM__.reset_index()\n",
    "\n",
    "tmp = pd.DataFrame(df_FPKM___.iloc[:,7])\n",
    "tmp[\"tmp1\"] = df_FPKM___.iloc[:,7]\n",
    "tmp[\"tmp2\"] = df_FPKM___.iloc[:,0]\n",
    "tmp[\"tmp3\"] = df_FPKM___.iloc[:,5]\n",
    "tmp[\"tmp4\"] = df_FPKM___.iloc[:,4]\n",
    "tmp[\"tmp5\"] = df_FPKM___.iloc[:,6]\n",
    "tmp[\"tmp6\"] = df_FPKM___.iloc[:,3]\n",
    "tmp[\"tmp7\"] = df_FPKM___.iloc[:,2]\n",
    "\n",
    "tmp.columns = ['KEGG_GENE', 'KEGG_PATH', 'NCBI', 'FPKM_log2', 'ENSG_no_var', 'Gene_name', 'Type', 'index']\n",
    "tmp = tmp.sort_values(by=[\"Type\"], ascending=True)\n",
    "\n",
    "df_template = pd.concat([df_link_, tmp])\n",
    "df_template = df_template.drop('FPKM_log2', axis = 1)\n",
    "df_template = df_template[df_template['ENSG_no_var'] != \"NA\"]\n",
    "df_template['KEGG_PATH'] = df_template['KEGG_PATH'].astype(int)\n",
    "\n",
    "df_template.to_csv(home_dir + '/info/template_308_genes.tsv', index=False, sep='\\t')\n",
    "\n",
    "tmp1 = df_template[df_template['KEGG_PATH'] != 0]\n",
    "tmp1 = tmp1.drop_duplicates(subset='ENSG_no_var',keep=\"first\")\n",
    "\n",
    "data_No = tmp1['ENSG_no_var'].size\n",
    "unique_gene_No = tmp1['ENSG_no_var'].unique().size\n",
    "temp_size = df_FPKM[\"FPKM\"].max()\n",
    "\n",
    "for j in range(batch_rand):\n",
    "    print(j)\n",
    "    img_name = home_dir + '/array_select_309/img_' + str(j) + '.tsv'\n",
    "    df_template = tmp1.take(np.random.permutation(len(tmp1)))\n",
    "    img = temp_size * np.ones((X_size, Y_size))\n",
    "    for i in range(data_No):\n",
    "        img[i, 0] = df_template.iloc[i,6]\n",
    "    pd.DataFrame(img).astype('int32').to_csv(img_name, index=False, sep='\\t')\n",
    "\n",
    "X_size = X_size_\n",
    "Y_size = Y_size_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP v6, 729 genes\n",
    "\n",
    "###### Making template from KEGG pathway information files\n",
    "# only primary tumor samples\n",
    "# TCGA project: 9,911 samples\n",
    "# major pathogolic types from GDC database: 11,997 samples\n",
    "\n",
    "# 60483 features\n",
    "# using only 308 genes selected by Pagging method\n",
    "\n",
    "# making 308 x 1 pxls images with 32 patterns\n",
    "\n",
    "if not os.path.exists(home_dir + '/array_top_v6'):\n",
    "    os.mkdir(home_dir + '/array_top_v6/')\n",
    "if not os.path.exists(home_dir + '/image_top_v6'):\n",
    "    os.mkdir(home_dir + '/image_top_v6')\n",
    "if not os.path.exists(home_dir + '/top_v6'):\n",
    "    os.mkdir(home_dir + '/top_v6')\n",
    "\n",
    "X_size_ = X_size\n",
    "Y_size_ = Y_size\n",
    "X_size = 729\n",
    "Y_size = 1\n",
    "\n",
    "df_genes = pd.read_table(home_dir + '/KEGG/genes.tsv', header=None)\n",
    "df_link = pd.read_table(home_dir + '/KEGG/link.tsv', header=None)\n",
    "df_pathway = pd.read_table(home_dir + '/KEGG/pathway.tsv', header=None)\n",
    "df_ncbi_geneid = pd.read_table(home_dir + '/KEGG/ncbi_geneid.tsv', header=None)\n",
    "df_ENSG = pd.read_table(home_dir + '/info/ENSG_to_NCBI.tsv', header=0, index_col=0)\n",
    "df_FPKM = pd.read_table(home_dir + '/info/FPKM_test.tsv', header=None)\n",
    "df_select = pd.read_table(home_dir + '/info/top_v6_729_genes.tsv', header=None)\n",
    "\n",
    "\n",
    "df_FPKM.columns = ['ENSG', 'FPKM']\n",
    "df_FPKM[\"NCBI\"] = \"NA\"\n",
    "df_FPKM[\"Type\"] = \"NA\"\n",
    "df_FPKM[\"ENSG_no_ver\"] = \"NA\"\n",
    "df_FPKM[\"FPKM_log2\"] = np.log2(df_FPKM[\"FPKM\"]+1)\n",
    "df_FPKM[\"Gene_name\"] = \"NA\"\n",
    "df_FPKM = df_FPKM[df_FPKM['ENSG'].str.split('.', expand=True)[0].isin(list(df_select[0]))]\n",
    "\n",
    "for i in range(len(df_FPKM[\"ENSG\"])):\n",
    "    if i % 500 ==0:\n",
    "        print(i)\n",
    "    df_FPKM.iloc[i,4] = df_FPKM.iloc[i,0].split(\".\")[0]\n",
    "    try:\n",
    "        tmp = df_ENSG.loc[df_FPKM.iloc[i,4]]\n",
    "    except:\n",
    "        continue\n",
    "    if tmp[\"entrezgene_id\"].size == 1:\n",
    "        df_FPKM.iloc[i,2] = tmp.iloc[2]\n",
    "        df_FPKM.iloc[i,3] = tmp.iloc[1]\n",
    "        df_FPKM.iloc[i,6] = tmp.iloc[0]\n",
    "    else:\n",
    "        for j in range(tmp[\"gene_biotype\"].size):\n",
    "            tmp2 = pd.Series([df_FPKM.iloc[i,0], df_FPKM.iloc[i,1], tmp.iloc[j,2], tmp.iloc[j,1], df_FPKM.iloc[i,4], df_FPKM.iloc[i,5], tmp.iloc[j,0]], index=['ENSG','FPKM','NCBI','Type','ENSG_no_ver','FPKM_log2','Gene_name'])\n",
    "            df_FPKM = df_FPKM.append([tmp2])\n",
    "\n",
    "df_ncbi_geneid.columns = ['KEGG', 'NCBI']\n",
    "for i in range(len(df_ncbi_geneid)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    df_ncbi_geneid.iloc[i,0] = df_ncbi_geneid.iloc[i,0].lstrip(\"hsa:\")\n",
    "    df_ncbi_geneid.iloc[i,1] = df_ncbi_geneid.iloc[i,1].lstrip(\"ncbi-geneid:\")\n",
    "\n",
    "df_ncbi_geneid_ = df_ncbi_geneid.set_index('KEGG')\n",
    "df_link.columns = ['KEGG_PATH', 'KEGG_GENE']\n",
    "df_link[\"NCBI\"] = \"NA\"\n",
    "\n",
    "for i in range(len(df_link)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    df_link.iloc[i,0] = df_link.iloc[i,0].lstrip(\"path:hsa\")\n",
    "    df_link.iloc[i,1] = df_link.iloc[i,1].lstrip(\"hsa:\")\n",
    "    try:\n",
    "        df_link.iloc[i,2] = df_ncbi_geneid_.loc[str(int(df_link.iloc[i,1]))].iloc[0]\n",
    "    except:\n",
    "        df_link.iloc[i,2] = \"NA\"\n",
    "\n",
    "df_FPKM_ = df_FPKM.copy()\n",
    "df_link_ = df_link.copy()\n",
    "df_link_[\"FPKM_log2\"] = 0\n",
    "df_link_[\"ENSG_no_var\"] = \"NA\"\n",
    "df_link_[\"Gene_name\"] = \"NA\"\n",
    "df_link_[\"Type\"] = \"NA\"\n",
    "df_link_[\"index\"] = 0\n",
    "df_FPKM_[\"Check\"] = 0\n",
    "df_FPKM_ = df_FPKM_.set_index('NCBI')\n",
    "\n",
    "for i in range(len(df_link_)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    try:\n",
    "        j = float(df_link_.iloc[i,2])\n",
    "        tmp = df_FPKM_.loc[j]\n",
    "        df_link_.iloc[i,3] = tmp.iloc[4]\n",
    "        df_link_.iloc[i,4] = tmp.iloc[3]\n",
    "        df_link_.iloc[i,5] = tmp.iloc[5]\n",
    "        df_link_.iloc[i,6] = tmp.iloc[2]\n",
    "        df_link_.iloc[i,7] = tmp.iloc[1]\n",
    "        df_FPKM_.loc[j,df_FPKM_.columns[6]] = 1\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_FPKM__ = df_FPKM_[df_FPKM_[\"Check\"] == 0].drop_duplicates(subset='ENSG', keep='last')\n",
    "df_FPKM___ = df_FPKM__.reset_index()\n",
    "\n",
    "tmp = pd.DataFrame(df_FPKM___.iloc[:,7])\n",
    "tmp[\"tmp1\"] = df_FPKM___.iloc[:,7]\n",
    "tmp[\"tmp2\"] = df_FPKM___.iloc[:,0]\n",
    "tmp[\"tmp3\"] = df_FPKM___.iloc[:,5]\n",
    "tmp[\"tmp4\"] = df_FPKM___.iloc[:,4]\n",
    "tmp[\"tmp5\"] = df_FPKM___.iloc[:,6]\n",
    "tmp[\"tmp6\"] = df_FPKM___.iloc[:,3]\n",
    "tmp[\"tmp7\"] = df_FPKM___.iloc[:,2]\n",
    "\n",
    "tmp.columns = ['KEGG_GENE', 'KEGG_PATH', 'NCBI', 'FPKM_log2', 'ENSG_no_var', 'Gene_name', 'Type', 'index']\n",
    "tmp = tmp.sort_values(by=[\"Type\"], ascending=True)\n",
    "\n",
    "df_template = pd.concat([df_link_, tmp])\n",
    "df_template = df_template.drop('FPKM_log2', axis = 1)\n",
    "df_template = df_template[df_template['ENSG_no_var'] != \"NA\"]\n",
    "df_template['KEGG_PATH'] = df_template['KEGG_PATH'].astype(int)\n",
    "\n",
    "df_template.to_csv(home_dir + '/info/template_top_v6.tsv', index=False, sep='\\t')\n",
    "\n",
    "tmp1 = df_template[df_template['KEGG_PATH'] != -1]\n",
    "tmp1 = tmp1.drop_duplicates(subset='ENSG_no_var',keep=\"first\")\n",
    "\n",
    "data_No = tmp1['ENSG_no_var'].size\n",
    "unique_gene_No = tmp1['ENSG_no_var'].unique().size\n",
    "temp_size = df_FPKM[\"FPKM\"].max()\n",
    "\n",
    "for j in range(batch_rand):\n",
    "    print(j)\n",
    "    img_name = home_dir + '/array_top_v6/img_' + str(j) + '.tsv'\n",
    "    df_template = tmp1.take(np.random.permutation(len(tmp1)))\n",
    "    img = temp_size * np.ones((X_size, Y_size))\n",
    "    for i in range(data_No):\n",
    "        img[i, 0] = df_template.iloc[i,6]\n",
    "    pd.DataFrame(img).astype('int32').to_csv(img_name, index=False, sep='\\t')\n",
    "\n",
    "X_size = X_size_\n",
    "Y_size = Y_size_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# STAR alignment and HTSeq read count of NCC dataset\n",
    "# STAR-2.7.1a\n",
    "# https://github.com/alexdobin/STAR/tree/2.7.1a\n",
    "# HTSeq-0.6.1p1 \n",
    "# https://pypi.python.org/packages/source/H/HTSeq/HTSeq-0.6.1p1.tar.gz\n",
    "# gencode.gene.info.v22.tsv, gencode.v22.annotation.gtf & GRCh38.d1.vd1.fa were obtained from GDC\n",
    "# https://gdc.cancer.gov/about-data/data-harmonization-and-generation/gdc-reference-files\n",
    "\n",
    "/mnt/HDD8TB/RNA_seq/run_STAR_HTSeq.sh /mnt/HDD/share/RNA_seq/sample_list.csv\n",
    "\n",
    "# convert HTSeq count files of NCC dataset consisting of 260 samples into FPKM values\n",
    "\n",
    "python3 count_to_FPKM.py \\\n",
    "             -c=\"/mnt/HDD8TB/RNA_seq/raw_count/*\" \\\n",
    "             -i=\"/mnt/HDD8TB/RNA_seq/gencode.gene.info.v22.tsv\" \\\n",
    "             -o=\"/mnt/HDD8TB/RNA_seq/FPKM_calc/FPKM\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# making images with RNA-seq data along with Hilbert curves\n",
    "# sorting with TCGA projects or pathologic types\n",
    "# CHANGE '/mnt/HDD/share/RNA_seq/FPKM_calc' to your home directory\n",
    "\n",
    "\n",
    "python3 alignment_1D.py \\\n",
    "             -o=\"/mnt/HDD8TB/RNA_seq/FPKM_calc/image_KEGG_256_128\" \\\n",
    "             -H=\"/mnt/HDD8TB/RNA_seq/FPKM_calc/array_KEGG_256_128\" \\\n",
    "             -F=\"/mnt/HDD8TB/RNA_seq/FPKM_calc/expression_data/*/*FPKM.txt.gz\" \\\n",
    "             -X=256 \\\n",
    "             -Y=128 &\n",
    "             \n",
    "\n",
    "python3 sorting_NCC.py \\\n",
    "             -f=\"/mnt/HDD/share/RNA_seq/FPKM_calc/NCC_file.tsv\" \\\n",
    "             -p=\"/mnt/HDD/share/RNA_seq/FPKM_calc/NCC_KEGG_patho\" \\\n",
    "             -F=\"/mnt/HDD/share/RNA_seq/FPKM_calc/NCC_image_KEGG_256_128/*.FPKM.txt.npy\"\n",
    "\n",
    "\n",
    "\n",
    "# combining GDC primary & meta dataset, and NCC dataset images into npz files\n",
    "\n",
    "python3 convert_npz.py\n",
    "\n",
    "\n",
    "python3 convert_npz_KEGG.py\n",
    "\n",
    "rm -r /mnt/HDD8TB/RNA_seq/FPKM_calc/mirna_1D_patho/other\n",
    "python3 convert_npz_mirna_1D.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### learning with 1D-CNNs\n",
    "# 32 CNNs and pagging\n",
    "\n",
    "if not os.path.exists(result_dir + \"/test_index\"):\n",
    "    os.mkdir(result_dir + \"/test_index\")\n",
    "if not os.path.exists(result_dir + \"/weight\"):\n",
    "    os.mkdir(result_dir + \"/weight\")\n",
    "if not os.path.exists(result_dir + \"/conf\"):\n",
    "    os.mkdir(result_dir + \"/conf\")\n",
    "if not os.path.exists(result_dir + \"/predict\"):\n",
    "    os.mkdir(result_dir + \"/predict\")\n",
    "if not os.path.exists(result_dir + \"/acc\"):\n",
    "    os.mkdir(result_dir + \"/acc\")\n",
    "if not os.path.exists(result_dir + \"/loss\"):\n",
    "    os.mkdir(result_dir + \"/loss\")\n",
    "\n",
    "batch_rand=32\n",
    "#NORMAL = \"_normal\"\n",
    "NORMAL = \"\"\n",
    "#n_categories = 46\n",
    "\n",
    "accuracy_table = []\n",
    "\n",
    "trainLabel = pd.read_table(data_dir + '_result' + \"/npy/label\" + NORMAL + \".tsv\", header=0)\n",
    "label_List = np.asarray(trainLabel).reshape(-1,)\n",
    "le = LabelEncoder()\n",
    "le = le.fit(label_List)\n",
    "label_List = le.transform(label_List)\n",
    "tmp = trainLabel[\"0\"].unique()\n",
    "tmp.sort()\n",
    "tmp1 = pd.DataFrame(tmp)\n",
    "cal_weights = cal_weight(tmp, data_dir)\n",
    "\n",
    "for bat in range(0, batch_rand):\n",
    "    kf = StratifiedKFold(n_splits=n_Splits, shuffle=True, random_state = seed1+bat)\n",
    "\n",
    "    image_list = np.load(data_dir + '_result' + \"/npy/image_\" + str(bat) + NORMAL + \".npz\")['arr_0'].astype(np.float32)\n",
    "    image_list = np.reshape(image_list, (-1, 1, X_size))\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # 5-fold cross validation\n",
    "    # train:valid:test = 60:20:20\n",
    "    # same separation in any batch\n",
    "    cv_batch = -1\n",
    "    for train_valid_index, test_index in kf.split(image_list, label_List):\n",
    "        cv_batch += 1\n",
    "        if(cv_batch != 5):\n",
    "            label_list = np_utils.to_categorical(label_List, n_categories)\n",
    "            np.save(result_dir + \"/test_index/cv_batch_\" + str(bat) + \"_\"  + str(cv_batch) + \"_index\"+ NORMAL + \".npy\", test_index)\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(image_list[train_valid_index], label_list[train_valid_index], random_state=seed2+bat, test_size=split, stratify=label_list[train_valid_index])\n",
    "            X_test = image_list[test_index]\n",
    "            y_test = label_list[test_index]\n",
    "\n",
    "            train_steps, train_batches = batch_iter(X_train, y_train, batch_size)\n",
    "            valid_steps, valid_batches = batch_iter(X_valid, y_valid, batch_size)\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            # training\n",
    "            inputs = Input(shape=(1, X_size))\n",
    "            x = Conv1D(32, 32, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(inputs)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=True)\n",
    "            #x = Conv1D(64, 16, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=True)\n",
    "            x = Conv1D(32, 128, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=True)\n",
    "            x = Conv1D(32, 256, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=True)\n",
    "            #x = Conv1D(64, 4, strides=4, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=True)\n",
    "            #x = Conv1D(128, 4, strides=4, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=True)\n",
    "\n",
    "\n",
    "            #x = Conv1D(512, 8, strides=8, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #x = Conv1D(256, 128, strides=128, padding='same', data_format=\"channels_first\", name='block1_conv2', kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=True)\n",
    "            #x = MaxPooling1D(pool_size=2, strides=None, padding='same', data_format=\"channels_first\")(x)\n",
    "\n",
    "            #x = Conv1D(32, 256, strides=1, padding='same', data_format=\"channels_first\", name='block1_conv3', kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=True)\n",
    "            x = Flatten(name='flatten')(x)\n",
    "            x = Dense(512, activation='relu', name='fc1')(x)\n",
    "            x = Dropout(0.2, name='dropout1')(x,training=True)\n",
    "\n",
    "\n",
    "            predictions = Dense(n_categories, activation='softmax', name='predictions')(x)\n",
    "            VGG_model = Model(inputs=inputs, outputs=predictions)\n",
    "            VGG_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=tf.keras.optimizers.SGD(lr=learning, momentum=0.9, decay=1e-4, nesterov=True),\n",
    "                      metrics=[\"acc\", Precision(), Recall()])\n",
    "            if bat == 0 and cv_batch == 0:\n",
    "                VGG_model.save_weights(result_dir + \"/weight/initial_weights\"+ NORMAL + \".h5\")\n",
    "            else:\n",
    "                VGG_model.load_weights(result_dir + \"/weight/initial_weights\"+ NORMAL + \".h5\")\n",
    "\n",
    "            gc.collect()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience , verbose=0)\n",
    "            checkpointer = ModelCheckpoint(monitor='val_loss', filepath = result_dir + \"/weight/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_model\"+ NORMAL + \".h5\", save_weights_only=True, verbose=0, mode='auto', save_best_only=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_delta=0.001, min_lr=0.000005)\n",
    "            print(\"\\rtraining: \" + str(bat) + \"_\" + str(cv_batch), end='')\n",
    "\n",
    "            history = VGG_model.fit_generator(train_batches, train_steps,\n",
    "                                epochs=epochs,\n",
    "                                class_weight=cal_weights,\n",
    "                                validation_data=valid_batches,\n",
    "                                validation_steps=valid_steps,\n",
    "                                verbose=1, \n",
    "                                callbacks=[reduce_lr, early_stopping, checkpointer])\n",
    "\n",
    "            del VGG_model\n",
    "            del x, inputs, predictions\n",
    "\n",
    "            inputs = Input(shape=(1, X_size))\n",
    "            x = Conv1D(32, 32, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(inputs)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=False)\n",
    "            #x = Conv1D(64, 16, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=False)\n",
    "            x = Conv1D(32, 128, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=False)\n",
    "            x = Conv1D(32, 256, strides=1, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=False)\n",
    "            #x = Conv1D(64, 4, strides=4, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=False)\n",
    "            #x = Conv1D(128, 4, strides=4, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=False)\n",
    "\n",
    "            #x = Conv1D(512, 8, strides=8, padding='same', data_format=\"channels_first\", kernel_initializer='he_normal')(x)\n",
    "            #x = ReLU(max_value=None)(x)\n",
    "            #x = BatchNormalization(axis=1)(x,training=False)\n",
    "\n",
    "            x = Flatten(name='flatten')(x)\n",
    "            x = Dense(512, activation='relu', name='fc1')(x)\n",
    "            x = Dropout(0.2, name='dropout1')(x,training=False)\n",
    "\n",
    "\n",
    "            predictions = Dense(n_categories, activation='softmax', name='predictions')(x)\n",
    "            VGG_model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "            VGG_model.load_weights(result_dir + \"/weight/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_model\"+ NORMAL + \".h5\")\n",
    "            y_pred = VGG_model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "            tmp2 = pd.DataFrame(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "            pd.concat([tmp1, tmp2], axis=1).to_csv(result_dir + \"/conf/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_conftable\"+ NORMAL + \".tsv\", index=False, sep='\\t')\n",
    "            np.save(result_dir + \"/predict/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_predict\"+ NORMAL + \".npy\", y_pred)\n",
    "\n",
    "            xlab = np.arange(0, len(history.history[\"acc\"])) + 1\n",
    "\n",
    "            # model evaluation\n",
    "            acc_txt = str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "            f1_txt = str(f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average=\"macro\"))\n",
    "            save_txt = 'Batch ' + str(bat) + \"_\" + str(cv_batch)  + ', Test accuracy/F1 score : ' + acc_txt + ' / ' + f1_txt + NORMAL + \"\\n\"\n",
    "            f = open(acc_name, 'a')\n",
    "            f.write(save_txt)\n",
    "            f.close()\n",
    "            accuracy_table.append([bat, cv_batch, accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)),f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average=\"macro\")])\n",
    "            pd.DataFrame(np.asarray(accuracy_table)).to_csv(acc_table, index=False, sep='\\t')\n",
    "\n",
    "\n",
    "            #plot accuracy\n",
    "            plt.plot(range(1, len(history.history[\"acc\"])+1), history.history[\"acc\"], label = \"Training acc\" )\n",
    "            plt.plot(range(1, len(history.history[\"val_acc\"])+1), history.history[\"val_acc\"], label = \"Validation acc\")\n",
    "            plt.title(\"Training and Validation accuracy\")\n",
    "            plt.legend()\n",
    "            plt.savefig(result_dir + \"/acc/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_acc\"+ NORMAL + \".png\")\n",
    "            plt.clf()\n",
    "\n",
    "            #plot loss\n",
    "            plt.plot(range(1, len(history.history[\"loss\"])+1), history.history[\"loss\"],  label = \"Training loss\" )\n",
    "            plt.plot(range(1, len(history.history[\"val_loss\"])+1), history.history[\"val_loss\"], label = \"Validation loss\")\n",
    "            plt.title(\"Training and Validation loss\")\n",
    "            plt.legend()\n",
    "            plt.savefig(result_dir + \"/loss/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_loss\"+ NORMAL + \".png\")\n",
    "            plt.clf()\n",
    "\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "            del VGG_model\n",
    "            del x, inputs, predictions\n",
    "            del history\n",
    "            del early_stopping\n",
    "            del checkpointer, reduce_lr\n",
    "            del X_train, X_valid, X_test\n",
    "            del train_steps, train_batches\n",
    "            del valid_steps, valid_batches\n",
    "\n",
    "\n",
    "# ensembling\n",
    "\n",
    "print(\"ensembling\")\n",
    "\n",
    "predict_ensemble = np.zeros((len(label_list),n_categories))\n",
    "\n",
    "for cv_batch in range(0, n_Splits):\n",
    "    for bat in range(0, batch_rand):\n",
    "        test_index = np.load(result_dir + \"/test_index/cv_batch_\" + str(bat) + \"_\"  + str(cv_batch) + \"_index\"+ NORMAL + \".npy\")\n",
    "        predict = np.load(result_dir + \"/predict/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_predict\"+ NORMAL + \".npy\")\n",
    "        for i in range(len(test_index)):\n",
    "            predict_ensemble[test_index] += predict[i]\n",
    "y_test = label_list\n",
    "tmp2 = pd.DataFrame(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1)))\n",
    "pd.concat([tmp1, tmp2], axis=1).to_csv(result_dir + \"/conf/ensemble_conftable\" + NORMAL + \".tsv\", index=False, sep='\\t')\n",
    "np.save(result_dir + \"/predict/ensemble_predict\"+ NORMAL + \".npy\", predict_ensemble)\n",
    "\n",
    "acc_txt = str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1)))\n",
    "f1_txt = str(f1_score(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1), average=\"macro\"))\n",
    "save_txt = 'Ensemble, Test accuracy/F1 score : ' + acc_txt + ' / ' + f1_txt+ NORMAL +  \"\\n\"\n",
    "f = open(acc_name, 'a')\n",
    "f.write(save_txt)\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### learning with 1D-CNNs\n",
    "# 32 CNNs and pagging\n",
    "# small scale\n",
    "\n",
    "batch_rand=32\n",
    "\n",
    "if not os.path.exists(result_dir + \"/test_index\"):\n",
    "    os.mkdir(result_dir + \"/test_index\")\n",
    "if not os.path.exists(result_dir + \"/weight\"):\n",
    "    os.mkdir(result_dir + \"/weight\")\n",
    "if not os.path.exists(result_dir + \"/conf\"):\n",
    "    os.mkdir(result_dir + \"/conf\")\n",
    "if not os.path.exists(result_dir + \"/predict\"):\n",
    "    os.mkdir(result_dir + \"/predict\")\n",
    "if not os.path.exists(result_dir + \"/acc\"):\n",
    "    os.mkdir(result_dir + \"/acc\")\n",
    "if not os.path.exists(result_dir + \"/loss\"):\n",
    "    os.mkdir(result_dir + \"/loss\")\n",
    "\n",
    "#batch_rand=32\n",
    "#NORMAL = \"_normal\"\n",
    "NORMAL = \"\"\n",
    "#n_categories = 33\n",
    "\n",
    "accuracy_table = []\n",
    "\n",
    "trainLabel = pd.read_table(data_dir + '_result' + \"/npy/label\" + NORMAL + \".tsv\", header=0)\n",
    "label_List = np.asarray(trainLabel).reshape(-1,)\n",
    "le = LabelEncoder()\n",
    "le = le.fit(label_List)\n",
    "label_List = le.transform(label_List)\n",
    "tmp = trainLabel[\"0\"].unique()\n",
    "tmp.sort()\n",
    "tmp1 = pd.DataFrame(tmp)\n",
    "\n",
    "for bat in range(0, batch_rand):\n",
    "    kf = StratifiedKFold(n_splits=n_Splits, shuffle=True, random_state = seed1)\n",
    "\n",
    "    image_list = np.load(data_dir + '_result' + \"/npy/image_\" + str(bat) + NORMAL + \".npz\")['arr_0'].astype(np.float32)\n",
    "    image_list = np.reshape(image_list, (-1, 1, X_size))\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # 5-fold cross validation\n",
    "    # train:valid:test = 60:20:20\n",
    "    # same separation in any batch\n",
    "    cv_batch = -1\n",
    "    for train_valid_index, test_index in kf.split(image_list, label_List):\n",
    "        cv_batch += 1\n",
    "        if(cv_batch != 5):\n",
    "\n",
    "            label_list = np_utils.to_categorical(label_List, n_categories)\n",
    "            np.save(result_dir + \"/test_index/cv_batch_\" + str(cv_batch) + \"_index\"+ NORMAL + \".npy\", test_index)\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(image_list[train_valid_index], label_list[train_valid_index], random_state=seed2+batch_rand, test_size=split, stratify=label_list[train_valid_index])\n",
    "            X_test = image_list[test_index]\n",
    "            y_test = label_list[test_index]\n",
    "\n",
    "            cal_weights = cal_weight(tmp, data_dir)\n",
    "            train_steps, train_batches = batch_iter(X_train, y_train, batch_size)\n",
    "            valid_steps, valid_batches = batch_iter(X_valid, y_valid, batch_size)\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            # training\n",
    "            inputs = Input(shape=(1, X_size))\n",
    "            x = Conv1D(32, 16, strides=1, padding='same', data_format=\"channels_first\", name='block1_conv1', kernel_initializer='he_normal')(inputs)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=True)\n",
    "            x = Conv1D(32, 64, strides=1, padding='same', data_format=\"channels_first\", name='block1_conv2', kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=True)\n",
    "            x = Conv1D(32, 128, strides=1, padding='same', data_format=\"channels_first\", name='block1_conv3', kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=True)\n",
    "\n",
    "            x = Flatten(name='flatten')(x)\n",
    "            x = Dense(512, activation='relu', name='fc1')(x)\n",
    "            x = Dropout(0.1, name='dropout1')(x,training=True)\n",
    "\n",
    "            predictions = Dense(n_categories, activation='softmax', name='predictions')(x)\n",
    "            VGG_model = Model(inputs=inputs, outputs=predictions)\n",
    "            VGG_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=tf.keras.optimizers.SGD(lr=learning, momentum=0.9, decay=1e-4, nesterov=True),\n",
    "                      #optimizer=tf.keras.optimizers.Adagrad(lr=learning, decay=1e-4),\n",
    "                      #optimizer=tf.keras.optimizers.Nadam(lr=learning, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                      metrics=[\"acc\", Precision(), Recall()])\n",
    "            if bat == 0 and cv_batch == 0:\n",
    "                VGG_model.save_weights(result_dir + \"/weight/initial_weights\"+ NORMAL + \".h5\")\n",
    "            else:\n",
    "                VGG_model.load_weights(result_dir + \"/weight/initial_weights\"+ NORMAL + \".h5\")\n",
    "\n",
    "            gc.collect()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience , verbose=0)\n",
    "            checkpointer = ModelCheckpoint(monitor='val_loss', filepath = result_dir + \"/weight/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_model\"+ NORMAL + \".h5\", save_weights_only=True, verbose=0, mode='min', save_best_only=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_delta=0.001, min_lr=0.00001)\n",
    "            print(\"\\rtraining: \" + str(bat) + \"_\" + str(cv_batch), end='')\n",
    "\n",
    "            history = VGG_model.fit_generator(train_batches, train_steps,\n",
    "                                epochs=epochs,\n",
    "                                class_weight=cal_weights,\n",
    "                                validation_data=valid_batches,\n",
    "                                validation_steps=valid_steps,\n",
    "                                verbose=1, \n",
    "                                callbacks=[reduce_lr, early_stopping, checkpointer])\n",
    "\n",
    "            del VGG_model\n",
    "            del x, inputs, predictions\n",
    "\n",
    "            inputs = Input(shape=(1, X_size))\n",
    "            x = Conv1D(32, 16, strides=1, padding='same', data_format=\"channels_first\", name='block1_conv1', kernel_initializer='he_normal')(inputs)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=False)\n",
    "            x = Conv1D(32, 64, strides=1, padding='same', data_format=\"channels_first\", name='block1_conv2', kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=False)\n",
    "            x = Conv1D(32, 128, strides=1, padding='same', data_format=\"channels_first\", name='block1_conv3', kernel_initializer='he_normal')(x)\n",
    "            x = ReLU(max_value=None)(x)\n",
    "            x = BatchNormalization(axis=1)(x,training=False)\n",
    "            x = Flatten(name='flatten')(x)\n",
    "            x = Dense(512, activation='relu', name='fc1')(x)\n",
    "            x = Dropout(0.1, name='dropout1')(x,training=False)\n",
    "\n",
    "            predictions = Dense(n_categories, activation='softmax', name='predictions')(x)\n",
    "            VGG_model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "            VGG_model.load_weights(result_dir + \"/weight/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_model\"+ NORMAL + \".h5\")\n",
    "            y_pred = VGG_model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "            tmp2 = pd.DataFrame(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "            pd.concat([tmp1, tmp2], axis=1).to_csv(result_dir + \"/conf/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_conftable\"+ NORMAL + \".tsv\", index=False, sep='\\t')\n",
    "            np.save(result_dir + \"/predict/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_predict\"+ NORMAL + \".npy\", y_pred)\n",
    "\n",
    "            xlab = np.arange(0, len(history.history[\"acc\"])) + 1\n",
    "\n",
    "            # model evaluation\n",
    "            acc_txt = str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "            f1_txt = str(f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average=\"macro\"))\n",
    "            save_txt = 'Batch ' + str(bat) + \"_\" + str(cv_batch)  + ', Test accuracy/F1 score : ' + acc_txt + ' / ' + f1_txt + NORMAL + \"\\n\"\n",
    "            f = open(acc_name, 'a')\n",
    "            f.write(save_txt)\n",
    "            f.close()\n",
    "            accuracy_table.append([bat, cv_batch, accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)),f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average=\"macro\")])\n",
    "            pd.DataFrame(np.asarray(accuracy_table)).to_csv(acc_table, index=False, sep='\\t')\n",
    "\n",
    "\n",
    "            #plot accuracy\n",
    "            plt.plot(range(1, len(history.history[\"acc\"])+1), history.history[\"acc\"], label = \"Training acc\" )\n",
    "            plt.plot(range(1, len(history.history[\"val_acc\"])+1), history.history[\"val_acc\"], label = \"Validation acc\")\n",
    "            plt.title(\"Training and Validation accuracy\")\n",
    "            plt.legend()\n",
    "            plt.savefig(result_dir + \"/acc/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_acc\"+ NORMAL + \".png\")\n",
    "            plt.clf()\n",
    "\n",
    "            #plot loss\n",
    "            plt.plot(range(1, len(history.history[\"loss\"])+1), history.history[\"loss\"],  label = \"Training loss\" )\n",
    "            plt.plot(range(1, len(history.history[\"val_loss\"])+1), history.history[\"val_loss\"], label = \"Validation loss\")\n",
    "            plt.title(\"Training and Validation loss\")\n",
    "            plt.legend()\n",
    "            plt.savefig(result_dir + \"/loss/rand_\" + str(bat) + \"_\" + str(cv_batch)  + \"_loss\"+ NORMAL + \".png\")\n",
    "            plt.clf()\n",
    "\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "            del VGG_model\n",
    "            del x, inputs, predictions\n",
    "            del history\n",
    "            del early_stopping\n",
    "            del checkpointer, reduce_lr\n",
    "            del cal_weights\n",
    "            del X_train, X_valid, X_test\n",
    "            del train_steps, train_batches\n",
    "            del valid_steps, valid_batches\n",
    "\n",
    "\n",
    "# ensembling\n",
    "\n",
    "print(\"ensembling\")\n",
    "\n",
    "for cv_batch in range(0,n_Splits):\n",
    "    test_index = np.load(result_dir + \"/test_index/cv_batch_\" + str(cv_batch) + \"_index\"+ NORMAL + \".npy\")\n",
    "    y_test = label_list[test_index]\n",
    "    predict_ensemble = np.load(result_dir + \"/predict/rand_\" + \"0\" + \"_\" + str(cv_batch) + \"_predict\"+ NORMAL + \".npy\")\n",
    "    for bat in range(1, batch_rand):\n",
    "        predict_ensemble += np.load(result_dir + \"/predict/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_predict\"+ NORMAL + \".npy\")\n",
    "    predict_ensemble /= batch_rand\n",
    "\n",
    "    tmp2 = pd.DataFrame(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1)))\n",
    "    pd.concat([tmp1, tmp2], axis=1).to_csv(result_dir + \"/conf/ensemble_conftable\" + \"_\" + str(cv_batch)+ NORMAL + \".tsv\", index=False, sep='\\t')\n",
    "    np.save(result_dir + \"/predict/ensemble_predict\" + \"_\" + str(cv_batch)+ NORMAL + \".npy\", predict_ensemble)\n",
    "\n",
    "    acc_txt = str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1)))\n",
    "    f1_txt = str(f1_score(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1), average=\"macro\"))\n",
    "    save_txt = 'Ensemble, Test accuracy/F1 score : ' + acc_txt + ' / ' + f1_txt+ NORMAL +  \"\\n\"\n",
    "    f = open(acc_name, 'a')\n",
    "    f.write(save_txt)\n",
    "    f.close()\n",
    "    accuracy_table.append([999, cv_batch, accuracy_score(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1)),f1_score(np.argmax(y_test, axis=1), np.argmax(predict_ensemble, axis=1), average=\"macro\")])\n",
    "    pd.DataFrame(np.asarray(accuracy_table)).to_csv(acc_table, index=False, sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy & confusion matrix\n",
    "\n",
    "#ACCURACY = pd.read_table(home_dir + \"/select_top1_patho_result/conf/rand_16_0_conftable.tsv\", header=0, index_col=0)\n",
    "#NORMAL = \"\"\n",
    "#DIRECTORY = home_dir + \"/select_patho_308_result/conf/ensemble_conftable_\"\n",
    "\n",
    "#NORMAL = \"_normal\"\n",
    "NORMAL = \"\"\n",
    "\n",
    "DIRECTORY = home_dir + \"/unique_1D_patho_result/conf/ensemble_conftable\"\n",
    "ACCURACY = pd.read_table(DIRECTORY + \".tsv\", header=0, index_col=0)\n",
    "\n",
    "#ACCURACY = pd.read_table(DIRECTORY + \"0\"  + NORMAL + \".tsv\", header=0, index_col=0)\n",
    "#for i in range(1,5):\n",
    "#    ACCURACY += pd.read_table(DIRECTORY + str(i)  + NORMAL +  \".tsv\", header=0, index_col=0)\n",
    "\n",
    "#cv_batch = 0\n",
    "#bat = 0\n",
    "#FILE = home_dir + \"/KEGG_1D_TCGA_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#FILE = home_dir + \"/unique_1D_KEGG_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#FILE = home_dir + \"/unique_1D_patho_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#ACCURACY = pd.read_table(FILE, header=0, index_col=0)\n",
    "#for cv_batch in range(1,5):\n",
    "#    FILE = home_dir + \"/KEGG_1D_TCGA_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#    FILE = home_dir + \"/unique_1D_KEGG_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#    FILE = home_dir + \"/unique_1D_patho_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#    ACCURACY += pd.read_table(FILE, header=0, index_col=0)\n",
    "\n",
    "#for bat in range(1,17):#batch_rand):\n",
    "#    for cv_batch in range(5):\n",
    "#        FILE = home_dir + \"/KEGG_1D_TCGA_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#        FILE = home_dir + \"/unique_1D_KEGG_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#        FILE = home_dir + \"/unique_1D_patho_result/conf/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_conftable.tsv\"\n",
    "#        ACCURACY += pd.read_table(FILE, header=0, index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CLASS = n_categories\n",
    "\n",
    "HIT = 0\n",
    "Recall = np.zeros(CLASS)\n",
    "Class_sum = np.zeros(CLASS)\n",
    "Precision = np.zeros(CLASS)\n",
    "Pred_sum = np.zeros(CLASS)\n",
    "F1_score = np.zeros(CLASS)\n",
    "Conf_matrix = np.zeros((CLASS,CLASS))\n",
    "TP = np.zeros(CLASS)\n",
    "TN = np.zeros(CLASS)\n",
    "FP = np.zeros(CLASS)\n",
    "FN = np.zeros(CLASS)\n",
    "MATTHEW = np.zeros(CLASS)\n",
    "for i in range(CLASS):\n",
    "    Class_sum[i] = np.sum(ACCURACY.values[i,:])\n",
    "    Pred_sum[i] = np.sum(ACCURACY.values[:,i])\n",
    "    HIT += ACCURACY.values[i][i]\n",
    "    TP[i] = ACCURACY.values[i][i]\n",
    "    FN[i] = Class_sum[i] - ACCURACY.values[i][i]\n",
    "    FP[i] = Pred_sum[i] - ACCURACY.values[i][i]\n",
    "    Recall[i] = ACCURACY.values[i][i] /  Class_sum[i]\n",
    "    Precision[i] = ACCURACY.values[i][i] / Pred_sum[i]\n",
    "    F1_score[i] = 2 * Recall[i] * Precision[i] / (Recall[i] + Precision[i])\n",
    "    Conf_matrix[i,:] = ACCURACY.values[i,:] / Class_sum[i]\n",
    "SUM = np.sum(Class_sum)\n",
    "TN = SUM - TP - FP - FN\n",
    "ACC = HIT/SUM\n",
    "RECALL = sum(Recall) / CLASS\n",
    "PRECISION = sum(Precision) / CLASS\n",
    "F1_SCORE = 2 * RECALL * PRECISION / (RECALL + PRECISION)\n",
    "MATTHEW = Matthew(TP,TN,FP,FN)\n",
    "MACRO_MATTHEW = np.sum(MATTHEW) / CLASS\n",
    "trainLabel = pd.read_table(data_dir + '_result' + \"/npy/label\" + NORMAL + \".tsv\", header=0)\n",
    "label_List = np.asarray(trainLabel).reshape(-1,)\n",
    "le = LabelEncoder()\n",
    "le = le.fit(label_List)\n",
    "label_List = le.transform(label_List)\n",
    "labels = trainLabel[\"0\"].unique()\n",
    "labels.sort()\n",
    "labels = labels\n",
    "tmp = []\n",
    "for i in range(len(labels)):\n",
    "    tmp.append(labels[i].replace(\"TCGA-\", \"\"))\n",
    "labels = tmp\n",
    "\n",
    "df_cmx = pd.DataFrame(Conf_matrix, index=labels, columns=labels)\n",
    "plt.clf()\n",
    "#plt.figure(figsize = (10,7))\n",
    "#plt.title('Confusion matrix of 33 TCGA projects' + NORMAL)\n",
    "plt.figure(figsize = (26,26))\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Confusion matrix of 46 cancer subtypes + Normal')\n",
    "plt.show(sns.heatmap(df_cmx, annot=None, cmap=\"Blues\", square=True))\n",
    "#plt.savefig(result_dir + \"/conf/heatmap_single_TCGA.png\")\n",
    "plt.savefig(result_dir + \"/conf/heatmap_TCGA\" + NORMAL + \".png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "ACCURACY.to_csv(result_dir + \"/conf/exsemble_total.tsv\", sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM visualisation\n",
    "# For 281 genes\n",
    "\n",
    "if not os.path.exists(result_dir + \"/Grad_CAM\"):\n",
    "    os.mkdir(result_dir + \"/Grad_CAM\")\n",
    "\n",
    "HilTable = pd.read_table(home_dir + \"/Hilbert/Hilbert_256_128.tsv\", header=0)\n",
    "df_template = pd.read_table(home_dir + '/info/template.tsv', header=0)\n",
    "df_pathway = pd.read_table(home_dir + '/KEGG/pathway.tsv', header=None)\n",
    "df_unique = pd.read_table(home_dir + '/array_KEGG_1D/img_0.tsv', header=0)\n",
    "df_image_temp = pd.read_table(home_dir + '/array_KEGG_256_128/img_0.tsv', header=0).values\n",
    "\n",
    "tmp1 = df_template[df_template['KEGG_PATH'] != 0]\n",
    "data_unique = tmp1.drop_duplicates()\n",
    "data_No = tmp1['ENSG_no_var'].size\n",
    "HilTable.columns = ['Num', 'X', 'Y']\n",
    "\n",
    "GCAM = np.zeros((n_categories, 256, 128))\n",
    "#GCAM = np.load(result_dir + \"/Grad_CAM/Grad_CAM_batch_18.npy\")\n",
    "\n",
    "\n",
    "ind_list = np.empty((0,2), int)\n",
    "important_gene = []\n",
    "important_pathway = []\n",
    "important_gene_id = []\n",
    "important_pathway_id = []\n",
    "\n",
    "\n",
    "NULL_list = []\n",
    "for i in range(X_size):\n",
    "    for j in range(Y_size):\n",
    "        if(df_unique.iloc[i,j] == 60483):\n",
    "            NULL_list.append([i,j])\n",
    "# len(NULL_list); 526\n",
    "\n",
    "\n",
    "trainLabel = pd.read_table(result_dir + \"/npy/label.tsv\", header=0)\n",
    "label_List = np.asarray(trainLabel).reshape(-1,)\n",
    "le = LabelEncoder()\n",
    "le = le.fit(label_List)\n",
    "label_List = le.transform(label_List)\n",
    "tmp = trainLabel[\"0\"].unique()\n",
    "tmp.sort()\n",
    "tmp1 = pd.DataFrame(tmp)\n",
    "\n",
    "for bat in range(0, batch_rand):\n",
    "    GCAM_batch = np.zeros((n_categories, X_size))\n",
    "    df_unique = pd.read_table(home_dir + '/array_KEGG_1D/img_' + str(bat) + '.tsv', header=0).values\n",
    "    image_list = np.load(result_dir + \"/npy/image_\" + str(bat) + \".npz\")['arr_0'].astype(np.float32)\n",
    "    image_list = np.reshape(image_list, (-1, 1, X_size))\n",
    "    for cv_batch in range(0, n_Splits):\n",
    "        test_index = np.load(result_dir + \"/test_index/cv_batch_\" + str(cv_batch) + \"_index.npy\")\n",
    "        X_test = image_list[test_index]\n",
    "        y_test = label_List[test_index]\n",
    "        inputs = Input(shape=(1, X_size))\n",
    "        x = Conv1D(32, 32, strides=1, activation='relu', padding='same', data_format=\"channels_first\", name='block1_conv1', kernel_initializer='he_normal')(inputs)\n",
    "        x = BatchNormalization(axis=1)(x,training=False)\n",
    "        x = Conv1D(16, 128, strides=1, activation='relu', padding='same', data_format=\"channels_first\", name='block1_conv2', kernel_initializer='he_normal')(x)\n",
    "        x = BatchNormalization(axis=1)(x,training=False)\n",
    "        x = Conv1D(2, 256, strides=1, activation='relu', padding='same', data_format=\"channels_first\", name='block1_conv3', kernel_initializer='he_normal')(x)\n",
    "        x = BatchNormalization(axis=1)(x,training=False)\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(512, activation='relu', name='fc1')(x)\n",
    "        x = Dropout(0.3, name='dropout1')(x,training=False)\n",
    "\n",
    "        predictions = Dense(n_categories, activation='softmax', name='predictions')(x)\n",
    "        VGG_model = Model(inputs=inputs, outputs=predictions)\n",
    "        VGG_model.load_weights(result_dir + \"/weight/rand_\" + str(bat) + \"_\" + str(cv_batch) + \"_model.h5\")\n",
    "        \n",
    "        for i in range(n_categories):\n",
    "            print(\"\\r\", bat, cv_batch, \"class: \", tmp[i], end=\"\")\n",
    "            Images = X_test[y_test == i]\n",
    "            Classes = y_test[y_test == i]\n",
    "            Layer_name = 'block1_conv3'\n",
    "            Input_model = VGG_model\n",
    "\n",
    "            for k in range((Classes.size + batch_size - 1) // batch_size):\n",
    "                start = k * batch_size\n",
    "                end = min((k+1) * batch_size, Classes.size)\n",
    "                GCAM_batch[i] += grad_batch(Input_model, Images[start:end], Classes[start:end], Layer_name, NULL_list, X_size, Y_size)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        del VGG_model\n",
    "        del x, inputs, predictions\n",
    "        del X_test, y_test\n",
    "    GCAM_temp = np.zeros((n_categories, 256, 128))\n",
    "    for i in range(256):\n",
    "        for j in range(128):\n",
    "            if(df_image_temp[i,j] != 60483):\n",
    "                tmp2 = HilTable[HilTable[\"X\"]==i]\n",
    "                tmp3 = tmp2[tmp2[\"Y\"]==j]\n",
    "                tmp4 = tmp3[\"Num\"]\n",
    "                tmp5 = math.ceil(tmp4 * data_No / 256 / 128)\n",
    "                tmp6 = data_unique[\"KEGG_PATH\"][tmp5]\n",
    "                X_ = np.where(df_unique == df_image_temp[i,j])[0]\n",
    "                for k in range(len(X_)):\n",
    "                    if(len(X_)==1):\n",
    "                        GCAM_temp[:,i,j] = GCAM_batch[:,X_].flatten()\n",
    "                    else:\n",
    "                        tmp11 = data_unique[\"KEGG_PATH\"][X_[k]]\n",
    "                        if(tmp11 == tmp6):\n",
    "                            GCAM_temp[:,i,j] = GCAM_batch[:,X_[k]].flatten()\n",
    "    GCAM += GCAM_temp\n",
    "    del image_list\n",
    "    np.save(result_dir + \"/Grad_CAM/Grad_CAM_batch\" + \"_\" + str(bat)+ \".npy\", GCAM)\n",
    "\n",
    "print(\"\\r\" + \"ensembling\")\n",
    "\n",
    "for i in range(n_categories):\n",
    "    picture = np.abs(GCAM[i]) / np.abs(GCAM[i]).max()\n",
    "    picture2 = Grad_imaging(picture)\n",
    "    picture2 = array_to_img(picture2)\n",
    "    plt.imshow(picture2)\n",
    "    plt.savefig(result_dir + \"/Grad_CAM/Grad_CAM_gene_\" + tmp[i] + \".png\")\n",
    "\n",
    "    gene_list = []\n",
    "    gene_id_list = []   \n",
    "    path_list = []\n",
    "    path_id_list = []\n",
    "\n",
    "    pathways = data_unique[\"KEGG_PATH\"].drop_duplicates().values.tolist()\n",
    "    path_index = 0\n",
    "    picture3 = picture\n",
    "    for j in range(200):\n",
    "        print(\"\\r\", \"category: \", i, \"gene: \", j, end=\"\")\n",
    "        ind_list = np.unravel_index(np.argsort(np.abs(picture.flatten()))[::-1][j], picture.shape)\n",
    "        gene_index = df_image_temp[ind_list[0]][ind_list[1]]\n",
    "        gene_list.append(data_unique[data_unique[\"index\"]==gene_index][\"Gene_name\"].drop_duplicates().values.tolist()[0])\n",
    "        gene_id_list.append(data_unique[data_unique[\"index\"]==gene_index][\"ENSG_no_var\"].drop_duplicates().values.tolist()[0])\n",
    "    important_gene.append(sorted(list(set(gene_list)),key=gene_list.index))\n",
    "    important_gene_id.append(sorted(list(set(gene_id_list)),key=gene_id_list.index))\n",
    "    tmp5 = np.zeros(330)\n",
    "    for j in range(330):\n",
    "        print(\"\\r\", \"category: \", i, \"pathway: \", j, end=\"\")\n",
    "        tmp2 = data_unique[data_unique[\"KEGG_PATH\"]==pathways[j]]\n",
    "        tmp3 = math.floor(path_index / data_No * 256 * 128)\n",
    "        tmp4 = math.floor((path_index + tmp2.shape[0]) / data_No * 256 * 128)\n",
    "        for k in range(tmp3,tmp4):\n",
    "            tmp5[j] += picture3[HilTable.iloc[k][\"X\"]][HilTable.iloc[k][\"Y\"]]\n",
    "        tmp5[j] /= tmp2.shape[0]\n",
    "        for k in range(tmp3,tmp4):\n",
    "            picture3[HilTable.iloc[k][\"X\"]][HilTable.iloc[k][\"Y\"]] = tmp5[j]\n",
    "        path_index += tmp2.shape[0]\n",
    "    for k in range(10):\n",
    "        path_list.append(df_pathway.iloc[np.abs(tmp5).argsort()[::-1][k]][1].replace(\" - Homo sapiens (human)\", \"\"))\n",
    "        path_id_list.append(df_pathway.iloc[tmp5.argsort()[::-1][k]][0])\n",
    "    picture3 = (picture3 - np.min(picture3)) / (np.max(picture3) - np.min(picture3))\n",
    "    picture4 = Grad_imaging(picture3)\n",
    "    picture4 = array_to_img(picture4)\n",
    "    plt.imshow(picture4)\n",
    "    plt.savefig(result_dir + \"/Grad_CAM/Grad_CAM_pathway_\" + tmp[i] + \".png\")\n",
    "    important_pathway.append(path_list)\n",
    "\n",
    "pd.concat([tmp1, pd.DataFrame(important_gene)], axis=1).to_csv(result_dir + \"/Grad_CAM/Grad_CAM_gene.tsv\", index=False, sep='\\t')\n",
    "pd.concat([tmp1, pd.DataFrame(important_pathway)], axis=1).to_csv(result_dir + \"/Grad_CAM/Grad_CAM_pathway.tsv\", index=False, sep='\\t')\n",
    "pd.concat([tmp1, pd.DataFrame(important_gene_id)], axis=1).to_csv(result_dir + \"/Grad_CAM/Grad_CAM_gene_id.tsv\", index=False, sep='\\t')\n",
    "pd.concat([tmp1, pd.DataFrame(important_pathway_id)], axis=1).to_csv(result_dir + \"/Grad_CAM/Grad_CAM_pathway_id.tsv\", index=False, sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM visualisation\n",
    "# For 1D, 7,666 genes\n",
    "batch_rand=32\n",
    "NORMAL = \"\"\n",
    "if not os.path.exists(result_dir + \"/Grad_CAM\"):\n",
    "    os.mkdir(result_dir + \"/Grad_CAM\")\n",
    "\n",
    "HilTable = pd.read_table(home_dir + \"/Hilbert/Hilbert_256_128.tsv\", header=0)\n",
    "df_template = pd.read_table(home_dir + '/info/template.tsv', header=0)\n",
    "df_pathway = pd.read_table(home_dir + '/KEGG/pathway.tsv', header=None)\n",
    "df_unique = pd.read_table(home_dir + '/array_unique_1D/img_0.tsv', header=0)\n",
    "df_image_temp = pd.read_table(home_dir + '/array_KEGG_256_128/img_0.tsv', header=0).values\n",
    "\n",
    "tmp1 = df_template[df_template['KEGG_PATH'] != 0]\n",
    "data_unique = tmp1.drop_duplicates()\n",
    "data_No = tmp1['ENSG_no_var'].size\n",
    "HilTable.columns = ['Num', 'X', 'Y']\n",
    "\n",
    "NULL_list = []\n",
    "for i in range(X_size):\n",
    "    for j in range(Y_size):\n",
    "        if(df_unique.iloc[i,j] == 60483):\n",
    "            NULL_list.append([i,j])\n",
    "# len(NULL_list); 526\n",
    "\n",
    "\n",
    "trainLabel = pd.read_table(result_dir + \"/npy/label\" + NORMAL + \".tsv\", header=0)\n",
    "label_List = np.asarray(trainLabel).reshape(-1,)\n",
    "le = LabelEncoder()\n",
    "le = le.fit(label_List)\n",
    "label_List = le.transform(label_List)\n",
    "tmp = trainLabel[\"0\"].unique()\n",
    "tmp.sort()\n",
    "tmp1 = pd.DataFrame(tmp)\n",
    "for bat in range(0, batch_rand):\n",
    "    GCAM = np.load(result_dir + \"/Grad_CAM/Grad_CAM_batch_\" + str(bat) + \".npy\")\n",
    "    ind_list = np.empty((0,2), int)\n",
    "    important_gene = []\n",
    "    important_gene_2 = []\n",
    "    important_pathway = []\n",
    "    important_gene_id = []\n",
    "    important_pathway_id = []\n",
    "\n",
    "    tmp5 = np.zeros((n_categories-1, 330))\n",
    "    tmp6 = np.zeros((n_categories-1, 200))\n",
    "    for i in range(n_categories-1):\n",
    "        picture = np.abs(GCAM[i]) / np.abs(GCAM[i]).max()\n",
    "\n",
    "        gene_list = []\n",
    "        gene_list_2 = []\n",
    "        gene_id_list = []   \n",
    "        path_list = []\n",
    "        path_id_list = []\n",
    "\n",
    "        pathways = data_unique[\"KEGG_PATH\"].drop_duplicates().values.tolist()\n",
    "        for j in range(200):\n",
    "            print(\"\\r\", \"category: \", i, \"gene: \", j, end=\"\")\n",
    "            ind_list = np.unravel_index(np.argsort(np.abs(picture.flatten()))[::-1][j], picture.shape)\n",
    "            gene_index = df_image_temp[ind_list[0]][ind_list[1]]\n",
    "            gene_list.append(data_unique[data_unique[\"index\"]==gene_index][\"Gene_name\"].drop_duplicates().values.tolist()[0])\n",
    "            gene_list_2.append(data_unique[data_unique[\"index\"]==gene_index][\"Gene_name\"].values.tolist()[0])\n",
    "        important_gene.append(sorted(list(set(gene_list)),key=gene_list.index))\n",
    "        important_gene_2.append(sorted(list(set(gene_list_2)),key=gene_list.index))\n",
    "        important_gene_2.append(np.sort(np.abs(picture.flatten()))[::-1][0:200])\n",
    "    pd.concat([tmp1, pd.DataFrame(important_gene)], axis=1).to_csv(result_dir + \"/Grad_CAM/Grad_CAM_gene\" + NORMAL + \"_\" + str(bat)+ \".tsv\", index=False, sep='\\t')\n",
    "    pd.concat([tmp1, pd.DataFrame(important_gene_2)], axis=1).to_csv(result_dir + \"/Grad_CAM/Grad_CAM_gene_full\" + NORMAL  + \"_\" + str(bat)+ \".tsv\", index=False, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
